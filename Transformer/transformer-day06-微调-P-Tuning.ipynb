{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型微调-P-Tuning\n",
    "\n",
    "prompt-tuning是一种近期的微调预训练语言模型的方法，重点是调整输入提示（input prompt）而非修改模型参数。这意味着预训练模型保持不变，只有输入提示被修改以适应下游的任务。通过设计和优化一组提示，可以使预训练模型执行特定任务。\n",
    "\n",
    "prompt-tuning和传统的fine-tuning的主要区别在于预训练模型被修改的程度。fine-tuning修改模型的权重，而提示调整只修改模型的输入。\n",
    "\n",
    "因此，prompt-tuning调整比精调的计算成本低，需要的资源和训练时间也更少。此外，prompt-tuning比精调更灵活，因为它允许创建特定任务的提示，可以适应各种任务。\n",
    "\n",
    "对于像GPT-3这样的大规模模型，整体精调可能需要大量计算资源。\n",
    "\n",
    "\n",
    "\n",
    "- 准备数据\n",
    "- 训练模型\n",
    "- 测试模型\n",
    "- 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "中文情感分析语料库 ChnSentiCorp 作为数据集，其包含各类网络评论接近一万条，可以从百度 ERNIE 示例仓库或者本仓库下载。\n",
    "\n",
    "语料已经划分好了训练集、验证集、测试集（分别包含 9600、1200、1200 条评论），一行是一个样本，使用 TAB 分隔评论和对应的标签，“0”表示消极，“1”表示积极。例如：\n",
    "\n",
    "```\n",
    "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。\t1\n",
    "\n",
    "```\n",
    "\n",
    "### 构建数据集\n",
    "首先编写继承自 Dataset 类的自定义数据集用于组织样本和标签。\n",
    "```\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                items = line.strip().split('\\t')\n",
    "                assert len(items) == 2\n",
    "                Data[idx] = {\n",
    "                    'comment': items[0], \n",
    "                    'label': items[1]\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = ChnSentiCorp('data/ChnSentiCorp/train.txt')\n",
    "valid_data = ChnSentiCorp('data/ChnSentiCorp/dev.txt')\n",
    "test_data = ChnSentiCorp('data/ChnSentiCorp/test.txt')\n",
    "```\n",
    "\n",
    "并且打印出一个训练样本：\n",
    "\n",
    "```\n",
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))\n",
    "\n",
    "\n",
    "train set size: 9600\n",
    "valid set size: 1200\n",
    "test set size: 1200\n",
    "{\n",
    "    'comment': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', \n",
    "    'label': '1'\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "最常见的 Prompting 方法就是借助模板将问题转换为 MLM 任务来解决。这里我们定义模板形式为“总体上来说很 [mask]。”，$x$其中$x$表示评论文本，\n",
    "\n",
    "并且规定如果[MASK]被预测为“好”就判定情感为“积极”，如果预测为“差”就判定为“消极”，即“积极”和“消极”标签对应的 label word 分别为“好”和“差”。\n",
    "\n",
    "处理数据:\n",
    "- 1、记录下模板中所有[MASK] token 位置，以便在模型的输出序列中将它们的表示取出。\n",
    "- 2、记录下 label word 对应的 token ID，因为我们实际上只关心模型在这些词语上的预测结果。\n",
    "\n",
    "下面我们首先编写模板和 verbalizer 对应的函数：\n",
    "```\n",
    "def get_prompt(x):\n",
    "    prompt = f'总体上来说很[MASK]。{x}'\n",
    "    return {\n",
    "        'prompt': prompt, \n",
    "        'mask_offset': prompt.find('[MASK]')\n",
    "    }\n",
    "\n",
    "def get_verbalizer(tokenizer):\n",
    "    return {\n",
    "        'pos': {'token': '好', 'id': tokenizer.convert_tokens_to_ids(\"好\")}, \n",
    "        'neg': {'token': '差', 'id': tokenizer.convert_tokens_to_ids(\"差\")}\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里由于模板中只包含一个[MASK] token，因此我们直接通过 str.find() 函数获取其位置，如果模板中包含多个[MASK]token，\n",
    "\n",
    "就需要把他们的位置都记录下来。verbalizer 记录了从标签到对应 label word 的映射，这里我们通过 tokenizer.convert_tokens_to_ids() 来获取 label word 对应的 token ID。例如，第一个样本转换后的模板为：\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "comment = '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "\n",
    "print('verbalizer:', get_verbalizer(tokenizer))\n",
    "\n",
    "prompt_data = get_prompt(comment)\n",
    "prompt, mask_offset = prompt_data['prompt'], prompt_data['mask_offset']\n",
    "\n",
    "encoding = tokenizer(prompt, truncation=True)\n",
    "tokens = encoding.tokens()\n",
    "mask_idx = encoding.char_to_token(mask_offset)\n",
    "\n",
    "print('prompt:', prompt)\n",
    "print('prompt tokens:', tokens)\n",
    "print('mask idx:', mask_idx)\n",
    "\n",
    "\n",
    "verbalizer: {'pos': {'token': '好', 'id': 1962}, 'neg': {'token': '差', 'id': 2345}}\n",
    "prompt: 总体上来说很[MASK]。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。\n",
    "prompt tokens: ['[CLS]', '总', '体', '上', '来', '说', '很', '[MASK]', '。', '这', '个', '宾', '馆', '比', '较', '陈', '旧', '了', '，', '特', '价', '的', '房', '间', '也', '很', '一', '般', '。', '总', '体', '来', '说', '一', '般', '。', '[SEP]']\n",
    "mask idx: 7\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，BERT 分词器正确地将“[MASK]”识别为一个 token，并且记录下[MASK]token 在序列中的索引。\n",
    "\n",
    "但是这种做法要求我们能够从词表中找到合适的 label word 来代表每一个类别，并且 label word 只能包含一个 token，而很多时候这是无法实现的。因此，另一种常见做法是为每个类别构建一个可学习的虚拟 token（又称伪 token），然后运用类别描述来初始化虚拟 token 的表示，最后使用这些虚拟 token 来扩展模型的 MLM 头。\n",
    "\n",
    "例如，这里我们可以为“积极”和“消极”构建专门的虚拟 token “[POS]”和“[NEG]”，并且设置对应的类别描述为“好的、优秀的、正面的评价、积极的态度”和“差的、糟糕的、负面的评价、消极的态度”。下面我们扩展一下上面的 verbalizer 函数，添加一个 vtype 参数来区分两种 verbalizer 类型：\n",
    "\n",
    "```\n",
    "def get_verbalizer(tokenizer, vtype):\n",
    "    assert vtype in ['base', 'virtual']\n",
    "    return {\n",
    "        'pos': {'token': '好', 'id': tokenizer.convert_tokens_to_ids(\"好\")}, \n",
    "        'neg': {'token': '差', 'id': tokenizer.convert_tokens_to_ids(\"差\")}\n",
    "    } if vtype == 'base' else {\n",
    "        'pos': {\n",
    "            'token': '[POS]', 'id': tokenizer.convert_tokens_to_ids(\"[POS]\"), \n",
    "            'description': '好的、优秀的、正面的评价、积极的态度'\n",
    "        }, \n",
    "        'neg': {\n",
    "            'token': '[NEG]', 'id': tokenizer.convert_tokens_to_ids(\"[NEG]\"), \n",
    "            'description': '差的、糟糕的、负面的评价、消极的态度'\n",
    "        }\n",
    "    }\n",
    "\n",
    "vtype = 'virtual'\n",
    "# add label words\n",
    "if vtype == 'virtual':\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[POS]', '[NEG]']})\n",
    "print('verbalizer:', get_verbalizer(tokenizer, vtype=vtype))\n",
    "\n",
    "\n",
    "verbalizer: {\n",
    "    'pos': {'token': '[POS]', 'id': 21128, 'description': '好的、优秀的、正面的评价、积极的态度'}, \n",
    "    'neg': {'token': '[NEG]', 'id': 21129, 'description': '差的、糟糕的、负面的评价、消极的态度'}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：“[POS]”和“[NEG]”是我们新添加的 token，因此如模型与分词器中介绍的那样，\n",
    "我们首先需要通过 tokenizer.add_special_tokens() 将这两个 token 添加进模型的词表，然后才能获取它们的 token ID。\n",
    "\n",
    "Prompting 方法实际输入的是转换后的模板，而不是原始文本，因此我们首先使用模板函数 get_prompt() 来更新数据集：\n",
    "\n",
    "```\n",
    "class ChnSentiCorp(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                items = line.strip().split('\\t')\n",
    "                assert len(items) == 2\n",
    "                prompt_data = get_prompt(items[0])\n",
    "                Data[idx] = {\n",
    "                    'comment': items[0], \n",
    "                    'prompt': prompt_data['prompt'], \n",
    "                    'mask_offset': prompt_data['mask_offset'], \n",
    "                    'label': items[1]\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "```\n",
    "\n",
    "同样地，我们通过 print(next(iter(train_data))) 打印出一个训练样本：\n",
    "\n",
    "```\n",
    "{\n",
    "    'comment': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', \n",
    "    'prompt': '总体上来说很[MASK]。选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', \n",
    "    'mask_offset': 6, \n",
    "    'label': '1'\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "可以看到输出的是转换后的模板，并且标记出了[MASK]在文本中的位置，符合我们的预期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "接下来我们就通过 DataLoader 库来按批(batch)加载数据，将文本转换为模型可以接受的 token IDs。\n",
    "```\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "vtype = 'base'\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "if vtype == 'virtual':\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[POS]', '[NEG]']})\n",
    "\n",
    "verbalizer = get_verbalizer(tokenizer, vtype='base')\n",
    "pos_id, neg_id = verbalizer['pos']['id'], verbalizer['neg']['id']\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentences, batch_mask_idxs, batch_labels  = [], [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentences.append(sample['prompt'])\n",
    "        encoding = tokenizer(sample['prompt'], truncation=True)\n",
    "        mask_idx = encoding.char_to_token(sample['mask_offset'])\n",
    "        assert mask_idx is not None\n",
    "        batch_mask_idxs.append(mask_idx)\n",
    "        batch_labels.append(int(sample['label']))\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentences, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    label_word_id = [neg_id, pos_id]\n",
    "    return {\n",
    "        'batch_inputs': batch_inputs, \n",
    "        'batch_mask_idxs': batch_mask_idxs, \n",
    "        'label_word_id': label_word_id, \n",
    "        'labels': batch_labels\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_data['batch_inputs'].items()})\n",
    "print(batch_data['batch_inputs'])\n",
    "print(batch_data['batch_mask_idxs'])\n",
    "print(batch_data['label_word_id'])\n",
    "print(batch_data['labels'])\n",
    "\n",
    "print:\n",
    "\n",
    "batch_X shape: {\n",
    "    'input_ids': torch.Size([4, 201]), \n",
    "    'token_type_ids': torch.Size([4, 201]), \n",
    "    'attention_mask': torch.Size([4, 201])\n",
    "}\n",
    "{\n",
    "    'input_ids': tensor([\n",
    "        [ 101, 2600,  860,  677, 3341, 6432, 2523,  103,  511, 6862, 2428, 2923,\n",
    "         ...],\n",
    "        [ 101, 2600,  860,  677, 3341, 6432, 2523,  103,  511, 3193, 2218, 2682,\n",
    "         ...],\n",
    "        [ 101, 2600,  860,  677, 3341, 6432, 2523,  103,  511, 2523, 4788, 4638,\n",
    "         ...],\n",
    "        [ 101, 2600,  860,  677, 3341, 6432, 2523,  103,  511, 3119, 1168, 6573,\n",
    "         ...]\n",
    "    ]), \n",
    "    'token_type_ids': tensor([...]), \n",
    "    'attention_mask': tensor([...])\n",
    "}\n",
    "[7, 7, 7, 7]\n",
    "[2345, 1962]\n",
    "[0, 1, 1, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "### 构建模型\n",
    "对于 MLM 任务，可以直接使用 Transformers 库封装好的 AutoModelForMaskedLM 类。由于 BERT 已经在 MLM 任务上进行了预训练，因此借助模板我们甚至可以在不微调的情况下 (Zero-shot) 直接使用模型来预测情感极性。例如对我们的第一个样本：\n",
    "\n",
    "```\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "text = \"总体上来说很[MASK]。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
    "\n",
    "\n",
    "'>>> 总体上来说很好。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "'>>> 总体上来说很棒。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "'>>> 总体上来说很差。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "'>>> 总体上来说很般。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "'>>> 总体上来说很赞。这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般。'\n",
    "```\n",
    "\n",
    "可以看到，BERT 模型成功地将 [MASK] token 预测成了我们预期的表意词“好”。这里我们还打印出了其他几个大概率的预测词，大部分都具有积极的情感（“好”、“棒”、“赞”）。\n",
    "\n",
    "当然，这种方式不够灵活，因此像之前章节中一样，本文采用继承 Transformers 库预训练模型的方式来手工构建模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "def batched_index_select(input, dim, index):\n",
    "    for i in range(1, len(input.shape)):\n",
    "        if i != dim:\n",
    "            index = index.unsqueeze(i)\n",
    "    expanse = list(input.shape)\n",
    "    expanse[0] = -1\n",
    "    expanse[dim] = -1\n",
    "    index = index.expand(expanse)\n",
    "    return torch.gather(input, dim, index)\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "class BertForPrompt(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "    \n",
    "    def forward(self, batch_inputs, batch_mask_idxs, label_word_id, labels=None):\n",
    "        bert_output = self.bert(**batch_inputs)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        batch_mask_reps = batched_index_select(sequence_output, 1, batch_mask_idxs.unsqueeze(-1)).squeeze(1)\n",
    "        prediction_scores = self.cls(batch_mask_reps)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(prediction_scores, labels)\n",
    "        return loss, prediction_scores[:, label_word_id]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForPrompt.from_pretrained(checkpoint, config=config).to(device)\n",
    "if vtype == 'virtual':\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"initialize embeddings of {verbalizer['pos']['token']} and {verbalizer['neg']['token']}\")\n",
    "    with torch.no_grad():\n",
    "        pos_tokenized = tokenizer(verbalizer['pos']['description'])\n",
    "        pos_tokenized_ids = tokenizer.convert_tokens_to_ids(pos_tokenized)\n",
    "        neg_tokenized = tokenizer(verbalizer['neg']['description'])\n",
    "        neg_tokenized_ids = tokenizer.convert_tokens_to_ids(neg_tokenized)\n",
    "        new_embedding = model.bert.embeddings.word_embeddings.weight[pos_tokenized_ids].mean(axis=0)\n",
    "        model.bert.embeddings.word_embeddings.weight[pos_id, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "        new_embedding = model.bert.embeddings.word_embeddings.weight[neg_tokenized_ids].mean(axis=0)\n",
    "        model.bert.embeddings.word_embeddings.weight[neg_id, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "Using cpu device\n",
    "initialize embeddings of [POS] and [NEG]\n",
    "BertForPrompt(\n",
    "  (bert): BertModel()\n",
    "  (cls): BertOnlyMLMHead(\n",
    "    (predictions): BertLMPredictionHead(\n",
    "      (transform): BertPredictionHeadTransform(\n",
    "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (transform_act_fn): GELUActivation()\n",
    "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      )\n",
    "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里为了能够加载预训练好的 MLM head 参数，我们按照 Transformers 库中的模型结构来构建 BertForPrompt 模型。可以看到，BERT 自带的 MLM head 由两个部分组成：\n",
    "\n",
    "首先对所有 token 进行一个 768X768 的非线性映射（包括激活函数和 LayerNorm），然后使用一个 768X21128 的线性映射预测词表中每个 token 的分数。\n",
    "\n",
    "如果采用虚拟 label word，我们除了向模型词表中添加“[POS]”和“[NEG]” token 以外， 还按照我们在 verbalizer 中设置的描述来初始化这两个 token 的嵌入。\n",
    "\n",
    "这里我们首先运用分词器将描述文本转换为对应的 token 列表 $t_1,t_2,...t_n$，然后初始化对应的表示为这些 token 嵌入的平均 $\\frac{1}{n}\\sum_{i = 1} ^n(E(t_i))$ 就是模型的 token embedding 矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 BERT 原始词表包含 21128 个 token，因此最终 MLM head 进行的是一个 21128 类的分类任务。如果我们设置 vtype = 'virtual' 采用虚拟 token 版本的 verbalizer，就会向词表中添加“[POS]”和“[NEG]”使得词表大小增加到 21130，此时再次运行上面的代码，就会看到输出张量大小 out_features 变为 21130 了。\n",
    "\n",
    "**注意**\n",
    "\n",
    "向模型词表中添加 token 包含两个步骤：\n",
    "\n",
    "通过 tokenizer.add_special_tokens() 向分词器中添加 token。这样分词器就能够在分词时将这些词分为独立的 token；\n",
    "\n",
    "通过 model.resize_token_embeddings() 扩展模型的词表大小。\n",
    "\n",
    "这两个步骤缺一不可！否则运行时就会出现错误。这里我们判断如果采用虚拟 label word，就调整模型词表大小。\n",
    "\n",
    "**注意**\n",
    "\n",
    "与之前相比，本次我们构建的 BertForPrompt 模型中增加了两个特殊的函数：get_output_embeddings() 和 set_output_embeddings()，负责调整模型的 MLM head。\n",
    "\n",
    "如果删除这两个函数，那么在调用 model.resize_token_embeddings() 时，就仅仅会调整模型词表的大小，而不会调整 MLM head，即运行上面的代码输出的张量维度依然是 21128。如果你不需要预测新添加 token 在 mask 位置的概率，那么即使删除这两个函数，代码也能正常运行，但是对于本文这种需要预测的情况就不行了。\n",
    "\n",
    "因此，在绝大部分情况下，你都应该添加这两个函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让模型适配我们的任务，这里首先通过 batched_index_select 函数从 BERT 的输出序列中抽取出 [MASK]token 对应的表示，在运用 MLM head 预测出该 \n",
    "\n",
    "[MASK]token 对应词表中每个 token 的分数之后，我们只返回类别对应 label words 的分数用于分类。\n",
    "\n",
    "为了测试模型的操作是否符合预期，我们尝试将一个 batch 的数据送入模型：\n",
    "\n",
    "```\n",
    "def to_device(batch_data):\n",
    "    new_batch_data = {}\n",
    "    for k, v in batch_data.items():\n",
    "        if k == 'batch_inputs':\n",
    "            new_batch_data[k] = {\n",
    "                k_: v_.to(device) for k_, v_ in v.items()\n",
    "            }\n",
    "        elif k == 'label_word_id':\n",
    "            new_batch_data[k] = v\n",
    "        else:\n",
    "            new_batch_data[k] = torch.tensor(v).to(device)\n",
    "    return new_batch_data\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "batch_data = to_device(batch_data)\n",
    "_, outputs = model(**batch_data)\n",
    "print(outputs.shape)\n",
    "\n",
    "torch.Size([4, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化模型参数\n",
    "与之前一样，我们将每一轮 Epoch 分为“训练循环”和“验证/测试循环”，在训练循环中计算损失、优化模型参数，在验证/测试循环中评估模型性能。下面我们首先实现训练循环。\n",
    "\n",
    "因为对标签词的预测实际上就是对类别的预测，所以这里模型的输出与同义句判断任务中介绍过的普通文本分类模型完全一致，损失也同样是通过在类别预测和答案标签之间计算交叉熵：\n",
    "\n",
    "```\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = epoch * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = to_device(batch_data)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证/测试循环负责评估模型的性能。对于分类任务最常见的就是通过精确率、召回率、F1值 (P / R / F1) 指标来评估每个类别的预测性能，然后再通过宏/微 F1 值 (Macro-F1/Micro-F1) 来评估整体分类性能。\n",
    "\n",
    "这里我们借助机器学习包 sklearn 提供的 classification_report 函数来输出这些指标，例如：\n",
    "\n",
    "```\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [1, 1, 0, 1, 2, 1, 0, 2, 1, 1, 0, 1, 0]\n",
    "y_pred = [1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "print(classification_report(y_true, y_pred, output_dict=False))\n",
    "\n",
    "\n",
    "   precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.75      0.60         4\n",
    "           1       0.67      0.57      0.62         7\n",
    "           2       1.00      0.50      0.67         2\n",
    "\n",
    "    accuracy                           0.62        13\n",
    "   macro avg       0.72      0.61      0.63        13\n",
    "weighted avg       0.67      0.62      0.62        13\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型\n",
    "与之前一样，我们会根据模型在验证集上的性能来调整超参数以及选出最好的模型权重，然后将选出的模型应用于测试集以评估最终的性能。这里我们继续使用 AdamW 优化器，并且通过 get_scheduler() 函数定义学习率调度器：\n",
    "\n",
    "```\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_f1_score = 0.\n",
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch+1}/{epoch_num}\\n\" + 30 * \"-\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, epoch, total_loss)\n",
    "    valid_scores = test_loop(valid_dataloader, model)\n",
    "    macro_f1, micro_f1 = valid_scores['macro avg']['f1-score'], valid_scores['weighted avg']['f1-score']\n",
    "    f1_score = (macro_f1 + micro_f1) / 2\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            f'epoch_{epoch+1}_valid_macrof1_{(macro_f1*100):0.3f}_microf1_{(micro_f1*100):0.3f}_model_weights.bin'\n",
    "        )\n",
    "print(\"Done!\")\n",
    "\n",
    "```\n",
    "\n",
    "在开始训练之前，我们先评估一下没有微调的 BERT 模型在测试集上的性能。\n",
    "```\n",
    "test_data = ChnSentiCorp('data/ChnSentiCorp/test.txt')\n",
    "test_dataloader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "test_loop(test_dataloader, model)\n",
    "\n",
    "\n",
    "pos: 53.05 / 100.00 / 69.33, neg: 100.00 / 9.12 / 16.72\n",
    "Macro-F1: 43.02 Micro-F1: 43.37\n",
    "```\n",
    "\n",
    "可以看到，得益于 Prompt 方法，不经微调的 BERT 模型也已经具有初步的情感分析能力，在测试集上的 Macro-F1 和 Micro-F1 值分别为 43.02 和 43.37。有趣的是，“积极”类别的召回率和“消极”类别的准确率都为 100%，这说明 BERT 对大部分样本都倾向于判断为“积极”类（可能预训练时看到的积极性文本更多吧）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "训练完成后，我们加载在验证集上性能最优的模型权重，汇报其在测试集上的性能。\n",
    "```\n",
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load('epoch_2_valid_macrof1_94.999_microf1_95.000_model_weights.bin'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    true_labels, predictions, probs = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        true_labels += batch_data['labels']\n",
    "        batch_data = to_device(batch_data)\n",
    "        outputs = model(**batch_data)\n",
    "        pred = outputs[1]\n",
    "        predictions += pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        probs += torch.nn.functional.softmax(pred, dim=-1)\n",
    "    save_resluts = []\n",
    "    for s_idx in tqdm(range(len(test_data))):\n",
    "        save_resluts.append({\n",
    "            \"comment\": test_data[s_idx]['comment'], \n",
    "            \"label\": true_labels[s_idx], \n",
    "            \"pred\": predictions[s_idx], \n",
    "            \"prob\": {'neg': probs[s_idx][0].item(), 'pos': probs[s_idx][1].item()}\n",
    "        })\n",
    "    metrics = classification_report(true_labels, predictions, output_dict=True)\n",
    "    pos_p, pos_r, pos_f1 = metrics['1']['precision'], metrics['1']['recall'], metrics['1']['f1-score']\n",
    "    neg_p, neg_r, neg_f1 = metrics['0']['precision'], metrics['0']['recall'], metrics['0']['f1-score']\n",
    "    macro_f1, micro_f1 = metrics['macro avg']['f1-score'], metrics['weighted avg']['f1-score']\n",
    "    print(f\"pos: {pos_p*100:>0.2f} / {pos_r*100:>0.2f} / {pos_f1*100:>0.2f}, neg: {neg_p*100:>0.2f} / {neg_r*100:>0.2f} / {neg_f1*100:>0.2f}\")\n",
    "    print(f\"Macro-F1: {macro_f1*100:>0.2f} Micro-F1: {micro_f1*100:>0.2f}\\n\")\n",
    "    print('saving predicted results...')\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for example_result in save_resluts:\n",
    "            f.write(json.dumps(example_result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "pos: 96.49 / 95.07 / 95.77, neg: 95.01 / 96.45 / 95.73\n",
    "Macro-F1: 95.75 Micro-F1: 95.75\n",
    "```\n",
    "\n",
    "可以看到，经过微调，模型在测试集上的 Macro-F1 值从 43.02 提升到 95.75，Micro-F1 值从 43.37 提升到 95.75，证明了我们对模型的微调是成功的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测函数\n",
    "我们训练模型的目的是为了能够给其他人提供服务。尤其对于不熟悉深度学习的普通开发者而言，需要的只是一个能够完成特定任务的接口。因此在大多数情况下，我们都应该将模型的预测过程封装为一个端到端 (End-to-End) 的函数：输入文本，输出结果：\n",
    "\n",
    "```\n",
    "def predict(model, tokenizer, comment, verbalizer):\n",
    "    prompt_data = get_prompt(comment)\n",
    "    prompt = prompt_data['prompt']\n",
    "    encoding = tokenizer(prompt, truncation=True)\n",
    "    mask_idx = encoding.char_to_token(prompt_data['mask_offset'])\n",
    "    assert mask_idx is not None\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        max_length=max_length, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {\n",
    "        'batch_inputs': inputs, \n",
    "        'batch_mask_idxs': [mask_idx], \n",
    "        'label_word_id': [verbalizer['neg']['id'], verbalizer['pos']['id']] \n",
    "    }\n",
    "    inputs = to_device(inputs)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[1]\n",
    "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    pred = logits.argmax(dim=-1)[0].item()\n",
    "    prob = prob[0][pred].item()\n",
    "    return pred, prob\n",
    "\n",
    "```\n",
    "下面我们尝试输出模型对测试集前 5 条数据的预测结果：\n",
    "```\n",
    "model.load_state_dict(torch.load('epoch_2_valid_macrof1_94.999_microf1_95.000_model_weights.bin'))\n",
    "\n",
    "for i in range(5):\n",
    "    data = test_data[i]\n",
    "    pred, prob = predict(model, tokenizer, data['comment'], verbalizer)\n",
    "    print(f\"{data['comment']}\\nlabel: {data['label']}\\tpred: {pred}\\tprob: {prob}\")\n",
    "\n",
    "\n",
    "这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般\n",
    "label: 1        pred: 0 prob: 0.9692065715789795\n",
    "怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片！开始还怀疑是不是赠送的个别现象，可是后来发现每张DVD后面都有！真不知道生产商怎么想的，我想看的是猫和老鼠，不是米老鼠！如果厂家是想赠送的话，那就全套米老鼠和唐老鸭都赠送，只在每张DVD后面添加一集算什么？？简直是画蛇添足！！\n",
    "label: 0        pred: 0 prob: 0.9736887216567993\n",
    "还稍微重了点，可能是硬盘大的原故，还要再轻半斤就好了。其他要进一步验证。贴的几种膜气泡较多，用不了多久就要更换了，屏幕膜稍好点，但比没有要强多了。建议配赠几张膜让用用户自己贴。\n",
    "label: 0        pred: 0 prob: 0.9987130165100098\n",
    "交通方便；环境很好；服务态度很好 房间较小\n",
    "label: 1        pred: 1 prob: 0.9942231774330139\n",
    "不错，作者的观点很颠覆目前中国父母的教育方式，其实古人们对于教育已经有了很系统的体系了，可是现在的父母以及祖父母们更多的娇惯纵容孩子，放眼看去自私的孩子是大多数，父母觉得自己的孩子在外面只要不吃亏就是好事，完全把古人几千年总结的教育古训抛在的九霄云外。所以推荐准妈妈们可以在等待宝宝降临的时候，好好学习一下，怎么把孩子教育成一个有爱心、有责任心、宽容、大度的人。\n",
    "label: 1        pred: 1 prob: 0.9959742426872253\n",
    "```\n",
    "可以看到，模型成功地输出了预测的类别和概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码\n",
    "https://colab.research.google.com/drive/1hL6no-VCOp3sZrx6XlnxA_btSu3UkMYx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- 1、https://transformers.run/nlp/2022-10-10-transformers-note-10.html\n",
    "- 2、https://zhuanlan.zhihu.com/p/620885226\n",
    "- 3、https://zhuanlan.zhihu.com/p/620618701"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
