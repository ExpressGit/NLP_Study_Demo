{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型微调-Frezee\n",
    "\n",
    "Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/620885226\n",
    "https://zhuanlan.zhihu.com/p/620618701\n",
    "\n",
    "- 加载数据集\n",
    "- 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集\n",
    "- 蚂蚁金融语义相似度数据集 AFQMC 作为语料，它提供了官方的数据划分，训练集 / 验证集 / 测试集分别包含 34334 / 4316 / 3861 个句子对，\n",
    "- 标签 0 表示非同义句，1 表示同义句\n",
    "\n",
    "数据格式：\n",
    "sentence1、sentence2 和 label 为键存储句子对和标签\n",
    "```\n",
    "{\"sentence1\": \"还款还清了，为什么花呗账单显示还要还款\", \"sentence2\": \"花呗全额还清怎么显示没有还款\", \"label\": \"1\"}\n",
    "```\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Pytorch 通过 Dataset 类和 DataLoader 类处理数据集和加载样本。同样地，这里我们首先继承 Dataset 类构造自定义数据集，以组织样本和标签\n",
    "\n",
    "因此我们使用 json 库按行读取样本，并且以行号作为索引构建数据集\n",
    "\n",
    "数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from torch.utils.data import IterableDataset\n",
    "import json\n",
    "\n",
    "class IterableAFQMC(IterableDataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.data_file, 'rt') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                yield sample\n",
    "\n",
    "train_data = IterableAFQMC('data/afqmc_public/train.json')\n",
    "\n",
    "print(next(iter(train_data)))\n",
    "\n",
    "\n",
    "{'sentence1': '蚂蚁借呗等额还款可以换成先息后本吗', 'sentence2': '借呗有先息到期还本吗', 'label': '0'}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "我们通过手工编写 DataLoader 的批处理函数 collate_fn 来实现。首先加载分词器，然后对每个 batch 中的所有句子对进行编码，同时把标签转换为张量格式：\n",
    "```\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)\n",
    "\n",
    "```\n",
    "\n",
    "可以看到，DataLoader 按照我们设置的 batch size 每次对 4 个样本进行编码，\n",
    "并且通过设置 padding=True 和 truncation=True 来自动对每个 batch 中的样本进行补全和截断。\n",
    "\n",
    "这里我们选择 BERT 模型作为 checkpoint所以每个样本都被处理成了\"[CLS]sen1[SEP]sen2[SEP]\"的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "#### 构建模型\n",
    "我们使用AutoModelForSequenceClassification 类微调。但是在实际操作中，除了使用预训练模型编码文本外，我们通常还会进行许多自定义操作，因此在大部分情况下我们都需要自己编写模型。\n",
    "\n",
    "最简单的方式是首先利用 Transformers 库加载 BERT 模型，然后接一个全连接层完成分类：\n",
    "\n",
    "```\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForPairwiseCLS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertForPairwiseCLS, self).__init__()\n",
    "        self.bert_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0, :]\n",
    "        cls_vectors = self.dropout(cls_vectors)\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits\n",
    "\n",
    "model = BertForPairwiseCLS().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "Using cpu device\n",
    "NeuralNetwork(\n",
    "  (bert_encoder): BertModel(\n",
    "    (embeddings): BertEmbeddings(...)\n",
    "    (encoder): BertEncoder(...)\n",
    "    (pooler): BertPooler(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (activation): Tanh()\n",
    "    )\n",
    "  )\n",
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "这里模型首先将输入送入到 BERT 模型中，将每一个 token 都编码为维度为 768 的向量，然后从输出序列中取出第一个 [CLS] token 的编码表示作为整个句子对的语义表示，再送入到一个线性全连接层中预测两个类别的分数。\n",
    "\n",
    "这种方式简单粗暴，但是相当于在 Transformers 模型外又包了一层，因此无法再调用 Transformers 库预置的模型函数。\n",
    "\n",
    "更为常见的写法是继承 Transformers 库中的预训练模型来创建自己的模型。例如这里我们可以继承 BERT 模型（BertPreTrainedModel 类）来创建一个与上面模型结构完全相同的分类器："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 推荐\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForPairwiseCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0, :]\n",
    "        cls_vectors = self.dropout(cls_vectors)\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForPairwiseCLS.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)\n",
    "\n",
    "```\n",
    "\n",
    "注意，此时我们的模型是 Transformers 预训练模型的子类，因此需要通过预置的 from_pretrained 函数来加载模型参数。\n",
    "\n",
    "这种方式也使得我们可以更灵活地操作模型细节，例如这里 Dropout 层就可以直接加载 BERT 模型自带的参数值，而不用像上面一样手工赋值。\n",
    "\n",
    "为了确保模型的输出符合我们的预期，我们尝试将一个 Batch 的数据送入模型：\n",
    "\n",
    "```\n",
    "outputs = model(batch_X)\n",
    "print(outputs.shape)\n",
    "\n",
    "torch.Size([4, 2])\n",
    "```\n",
    "可以看到模型输出了一个 4 x 2 的张量，符合我们的预期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化模型参数\n",
    "在训练模型时，我们将每一轮 Epoch 分为训练循环和验证/测试循环。在训练循环中计算损失、优化模型的参数，在验证/测试循环中评估模型的性能：\n",
    "\n",
    "```\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = (epoch-1)*len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for step, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /= size\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers 库同样实现了很多的优化器，并且相比 Pytorch 固定学习率，Transformers 库的优化器会随着训练过程逐步减小学习率（通常会产生更好的效果）。例如我们前面使用过的 AdamW 优化器：\n",
    "\n",
    "默认情况下，优化器会线性衰减学习率，对于上面的例子，学习率会线性地从5e-5降到 0。为了正确地定义学习率调度器，我们需要知道总的训练步数 (step)，它等于训练轮数 (Epoch number) 乘以每一轮中的步数（也就是训练 dataloader 的大小）：\n",
    "\n",
    "```\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    test_loop(valid_dataloader, model, mode='Valid')\n",
    "print(\"Done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存和加载模型\n",
    "\n",
    "这里我们在测试循环时返回计算出的准确率，然后对上面的 Epoch 训练代码进行小幅的调整，以保存验证集上准确率最高的模型：\n",
    "\n",
    "```\n",
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /= size\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct\n",
    "\n",
    "total_loss = 0.\n",
    "best_acc = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_acc = test_loop(valid_dataloader, model, mode='Valid')\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin')\n",
    "print(\"Done!\")\n",
    "\n",
    "```\n",
    "\n",
    "可以看到，随着训练的进行，在验证集上的准确率逐步提升（71.8% -> 72.0% -> 74.1%）。因此，3 轮 Epoch 训练结束后，会在目录下保存下所有三轮模型的权重：\n",
    "\n",
    "```\n",
    "epoch_1_valid_acc_71.8_model_weights.bin\n",
    "epoch_2_valid_acc_72.0_model_weights.bin\n",
    "epoch_3_valid_acc_74.1_model_weights.bin\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- 1、https://transformers.run/intro/2021-12-17-transformers-note-4/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
