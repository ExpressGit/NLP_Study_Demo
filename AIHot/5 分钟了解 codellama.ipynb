{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目概况：\n",
    "\n",
    "项目地址：https://github.com/facebookresearch/codellama\n",
    "\n",
    "### 1、 介绍说明\n",
    "\n",
    "最新版本的 Code Llama 现在可供个人、创作者、研究人员和各种规模的企业使用，以便他们能够负责任地实验、创新和扩展他们的想法。 \n",
    "\n",
    "此版本包括预训练和微调 Llama 语言模型的模型权重和起始代码 - 参数范围从 7B 到 34B。\n",
    "\n",
    "这是一个基于 Llama 2 的大型代码语言模型系列，在开放模型、填充功能、对大输入上下文的支持以及编程任务的零样本指令跟踪能力中提供了最先进的性能。 \n",
    "\n",
    "我们提供多种风格来覆盖广泛的应用程序：基础模型 (Code Llama)、Python 专业化 (Code Llama - Python) 和指令跟随模型 (Code Llama - Instruct)，每个模型都有 7B、13B 和 34B 参数。 \n",
    "\n",
    "### 2、商用规则\n",
    "需要得到 meta 授权 ，才能商用\n",
    "\n",
    "\n",
    "## 训练方式\n",
    "\n",
    "Meta 提供的 Code Llama 版本包括：\n",
    "\n",
    "- Code Llama，基础代码模型；\n",
    "- Code Llama-Python，Python 微调版；\n",
    "- Code Llama-Instruct，自然语言指令微调版。\n",
    "\n",
    "7B 和 13B Code Llama 和 Code Llama - 指令变体支持基于周围内容的填充。 Code Llama 在多个代码基准测试中达到了开放模型中最先进的性能，在 HumanEval 和 MBPP 上的得分分别高达 53% 和 55%。 \n",
    "\n",
    "所有模型均在 16k 个标记序列上进行训练，并在最多 100k 个标记的输入上显示出改进。 \n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "**值得注意的是，Code Llama - Python 7B 在 HumanEval 和 MBPP 上的性能优于 Llama 2 70B，并且我们所有的模型在 MultiPL-E 上都优于所有其他公开可用的模型。 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先决条件：确保安装了 wget 和 md5sum。 然后运行脚本：bash download.sh。 \n",
    "请记住，链接将在 24 小时和一定下载量后过期。 如果您开始看到诸如 403: Forbidden 之类的错误，您可以随时重新请求链接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sizes\n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "所有模型都支持高达 100,000 个 token 的序列长度，但我们根据 max_seq_len 和 max_batch_size 值预先分配缓存。\n",
    "\n",
    "因此，请根据您的硬件和用例进行设置。\n",
    "\n",
    "mp说明：\n",
    "https://pytorch.org/docs/stable/distributed.html\n",
    "\n",
    "MP relates to the model size:\n",
    "\n",
    "1 = 7B\n",
    "\n",
    "2 = 13B\n",
    "\n",
    "4 = 33B\n",
    "\n",
    "8 = 65B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署\n",
    "```python\n",
    "\n",
    "git clone https://github.com/facebookresearch/codellama\n",
    "\n",
    "cd codellama\n",
    "\n",
    "pip install -e .\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用预训练模型\n",
    "```python\n",
    "\n",
    "torchrun --nproc_per_node 1 example_completion.py \\\n",
    "    --ckpt_dir CodeLlama-7b/ \\\n",
    "    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n",
    "    --max_seq_len 128 --max_batch_size 4\n",
    "\n",
    "```\n",
    "可用模型：\n",
    "Pretrained code models are: \n",
    "\n",
    "the Code Llama models CodeLlama-7b, CodeLlama-13b, CodeLlama-34b \n",
    "\n",
    "and the Code Llama - Python models CodeLlama-7b-Python, CodeLlama-13b-Python, CodeLlama-34b-Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Infilling（代码填充）\n",
    "```python\n",
    "\n",
    "torchrun --nproc_per_node 1 example_infilling.py \\\n",
    "    --ckpt_dir CodeLlama-7b/ \\\n",
    "    --tokenizer_path CodeLlama-7b/tokenizer.model \\\n",
    "    --max_seq_len 192 --max_batch_size 4\n",
    "\n",
    "```\n",
    "可用模型：\n",
    "\n",
    "Pretrained infilling models are: \n",
    "\n",
    "the Code Llama models CodeLlama-7b and CodeLlama-13b \n",
    "\n",
    "and the Code Llama - Instruct models CodeLlama-7b-Instruct, CodeLlama-13b-Instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Instruction Models（微调）\n",
    "\n",
    "```python\n",
    "\n",
    "torchrun --nproc_per_node 1 example_instructions.py \\\n",
    "    --ckpt_dir CodeLlama-7b-Instruct/ \\\n",
    "    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 4\n",
    "\n",
    "```\n",
    "数据格式：\n",
    "\n",
    "a specific formatting defined in chat_completion needs to be followed\n",
    "\n",
    "including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and linebreaks in between (we recommend calling strip() on inputs to avoid double-spaces). \n",
    "\n",
    "You can use chat_completion directly to generate answers with the instruct model.\n",
    "\n",
    "\n",
    "可用模型：\n",
    "\n",
    "Fine-tuned instruction-following models are: \n",
    "\n",
    "the Code Llama - Instruct models CodeLlama-7b-Instruct, CodeLlama-13b-Instruct, CodeLlama-34b-Instruct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 体验\n",
    "\n",
    "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/playground/models/codellama\n",
    "\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "\n",
    "- 1、题目 1：买卖股票时机问题（https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/）\n",
    "- 2、题目 2：三角形最小路径和（https://leetcode.cn/problems/triangle/）\n",
    "- 3、题目 3：不同的子序列（https://leetcode.cn/problems/distinct-subsequences/）\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
